# Fault tolerance for eks deploy

**autero1** commented *Jan 6, 2022*

<!--
  Have any questions? Check out the contributing docs at https://docs.gruntwork.io/guides/contributing/, or
  ask in this Pull Request and a Gruntwork core maintainer will be happy to help :)
  Note: Remember to add '[WIP]' to the beginning of the title if this PR is still a work-in-progress.
-->

## Description

This PR introduces fault tolerance for the `eks deploy` subcommand by introducing a recovery file for storing deploy state.
The `eks deploy` is split into multiple stages and the state file is updated after each stage:

1. Gather ASG info: Retrieve the ASG object and gather required info
2. Set ASG max capacity: Make sure there is enough max size capacity to scale up
3. Scale up: Launching new nodes with new launch config
4. Wait for new nodes: Wait for new nodes to be ready and verify everything is OK in k8s
5. Cordon nodes: Cordon old instances in cluster ASG
6. Drain nodes: Drain Pods on old instances
7. Detach instances: Remove old nodes from ASG
8. Terminate instances: Terminating detached instances
9. Restore ASG max capacity: Restore the ASG into original max capacity

The state file is generated into the working directory `./.kubergrunt.state` and persisted after each successful stage. If the `eks deploy` command fails for some reason, execution resumes from the last successful state. The existing recovery file can also be ignored with the `--ignore-recovery-file` flag. In this case the recovery file will be re-initialized.

The state is implemented with simple boolean flags instead of a state machine library. The stages in `eks deploy` are linear and there is no complex state handling, so a state machine library seemed overkill.

To better modularity and keeping action and verification stages separate, the `scaleUp` function is split into actual scale up and verification functions. This allows safer re-execution in case scale up was successful but verification fails for some reason.

### Other notable things

- Removing `defer setAsgMaxSize` and moving that to the last stage. If we're retrying using the recovery file, the original `defer` functionality could potentially lead to unexpected behaviour. I'm open for discussion on this, though.

- Kubernetes `drain` and `cordon` operations are still executed for the whole set of existing nodes - I assume these are idempotent and can safely be executed multiple times.

- As the AWS operations for detaching and terminating instances are batched, the command can still fail if previous detach, for example, was a partial success. Same applies to termination. Don't necessarily think this will be an issue


### Documentation

- README updated
- cli option documented

## TODOs

- [x] Ensure the branch is named correctly with the issue number. e.g: `feature/new-vpc-endpoints-955` or `bug/missing-count-param-434`.
- [x] Update the docs.
- [x] Keep the changes backwards compatible where possible.
- [x] Run the pre-commit checks successfully.
- [x] Run the relevant tests successfully.
- [x] Ensure any 3rd party code adheres with our license policy: https://www.notion.so/gruntwork/Gruntwork-licenses-and-open-source-usage-policy-f7dece1f780341c7b69c1763f22b1378
- [ ] _Maintainers Only._ If necessary, release a new version of this repo.
- [ ] _Maintainers Only._ If there were backwards incompatible changes, include a migration guide in the release notes.
- [ ] _Maintainers Only._ Add to the next version of the monthly newsletter (see https://www.notion.so/gruntwork/Monthly-Newsletter-9198cbe7f8914d4abce23dca7b435f43).


## Related Issues

Fixes #109 

<br />
***


**autero1** commented *Jan 10, 2022*

Thanks for the review. I reworked the logic quite a bit - including encapsulating stages as methods on the state machine struct and changing from single ASG to multiple in the struct.  
***

**yorinasub17** commented *Jan 10, 2022*

@autero1 Here is the integration test: https://app.circleci.com/pipelines/github/gruntwork-io/terraform-aws-eks/1697/workflows/a783b4d0-afac-4b3d-92ec-b17c7efd20ae
***

**yorinasub17** commented *Jan 10, 2022*

The integration test revealed at least one bug in the state machine:

```
[] time="2022-01-10t21:49:15z" level=info msg="beginning roll out for eks cluster worker group eks-cluster-dnjdbl-additional-2022011021394957890000000d in us-east-1" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="successfully authenticated with aws" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="retrieving current asg info" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="successfully retrieved current asg info." name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="\tcurrent desired capacity: 2" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="\tcurrent max size: 4" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="\tcurrent capacity: 2" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="no max retries set. defaulted to 40 based on sleep between retries duration of 15s and scale up count 2." name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="starting with the following list of instances in asg:" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="i-068c52a32617f04e0,i-0d31992bc468dc970" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="launching new nodes with new launch config on asg eks-cluster-dnjdbl-additional-2022011021394957890000000d" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=info msg="updating asg eks-cluster-dnjdbl-additional-2022011021394957890000000d desired capacity to 0." name=kubergrunt
[] time="2022-01-10t21:49:15z" level=error msg="failed to set asg capacity to 0" name=kubergrunt
[] time="2022-01-10t21:49:15z" level=error msg="if the capacity is set in aws, undo by lowering back to the original capacity. if the capacity is not yet set, triage the error message below and try again." name=kubergrunt
[] time="2022-01-10T21:49:15Z" level=error msg="*awserr.requestError ValidationError: New SetDesiredCapacity value 0 is below min value 2 for the AutoScalingGroup.\n\tstatus code: 400, request id: 15dece0b-6ee9-4257-a56e-42358af28475\n/home/circleci/project/eks/asg.go:153 (0x15710b1)\n/home/circleci/project/eks/asg.go:46 (0x15703b5)\n/home/circleci/project/eks/deploy_state.go:203 (0x1578ab6)\n/home/circleci/project/eks/deploy.go:72 (0x1576ee5)\n/home/circleci/project/cmd/eks.go:419 (0x158a5b1)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:526 (0x5e36e5)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/command.go:174 (0x5e4459)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:407 (0x5e2454)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/command.go:373 (0x5e659f)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/command.go:102 (0x5e48f4)\n/home/circleci/.go_workspace/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:279 (0x5e1668)\n/home/circleci/.go_workspace/pkg/mod/github.com/gruntwork-io/go-commons@v0.8.2/entrypoint/entrypoint.go:35 (0x615a78)\n/home/circleci/project/cmd/main.go:66 (0x158c8a8)\n/usr/local/go/src/runtime/proc.go:225 (0x438ed6)\n\tmain: fn()\n/usr/local/go/src/runtime/asm_amd64.s:1371 (0x46c021)\n\tgoexit: BYTE\t$0x90\t// NOP\n" error="ValidationError: New SetDesiredCapacity value 0 is below min value 2 for the AutoScalingGroup.\n\tstatus code: 400, request id: 15dece0b-6ee9-4257-a56e-42358af28475"
 ```
***

**autero1** commented *Jan 11, 2022*

Yes, I think the problem is that `asg := state.ASGs[0]` is a value assignment rather than a reference. Since some of the stages mutate the `ASG` we should use the reference, instead:  `asg := &state.ASGs[0]`
***

**yorinasub17** commented *Jan 11, 2022*

Ah makes sense! Will cut a new test release and update the integration test branch.
***

**yorinasub17** commented *Jan 11, 2022*

Integration testing build is here btw: https://app.circleci.com/pipelines/github/gruntwork-io/terraform-aws-eks/1698/workflows/9e334364-7615-4c6d-8e8e-cfdc8c5d4485
***

**autero1** commented *Jan 12, 2022*

Not 100% sure the test failures were related. I kicked off the tests again.
https://app.circleci.com/pipelines/github/gruntwork-io/terraform-aws-eks/1698/workflows/4c25d48e-7f65-4dcc-a778-58290c6ad0ff
***

**autero1** commented *Jan 15, 2022*

I discovered a bug while testing this manually with an existing EKS cluster. 

Since `maxRetries` is needed (and calculated) in `gatherASGInfo`, the calculated default value was lost for subsequent invocations (using the recovery file) due to the skip condition. If `--max-retries` option was omitted, the value was never re-calculated and was always set to 0, obviously leading to any stage relying on that to fail.

I fixed that by creating a method `ensureMaxRetries` that is invoked even when `GatherASGInfoDone` is `true`. This allows the user to override the value across retries.

The previous EKS module test was successful and now I've verified the functionality manually step by step, as well.
***

**rhoboat** commented *Jan 15, 2022*

This is a cool feature. I look forward to reviewing it soon.
***

**yorinasub17** commented *Jan 18, 2022*

Just kicked off another regression build: https://app.circleci.com/pipelines/github/gruntwork-io/terraform-aws-eks/1709/workflows/a947a9a8-3124-4190-9c4b-8d4cbcc329c9

If that passes, I think we can merge this in!
***

**autero1** commented *Jan 21, 2022*

Thanks for the review! I removed the extra assertion, so think we're ready to release this now!
***

**autero1** commented *Jan 25, 2022*

Thanks for the reviews! Merging this in now!
***

